"""
Index manager extracted from legacy single-file implementation.
"""

import os
import time
import threading
import concurrent.futures
from collections import deque
from pathlib import Path
import logging

from PySide6.QtCore import QObject, Signal

from ..constants import (
    LOG_DIR,
    IS_WINDOWS,
    SKIP_DIRS_LOWER,
    SKIP_EXTS,
)
from ..utils import should_skip_path, should_skip_dir, get_c_scan_dirs
from ..utils import format_size, format_time  # scoring utilities removed
from .dependencies import HAS_APSW, get_db_module
from .mft_scanner import enum_volume_files_mft
from .trigram_index import TrigramIndex

logger = logging.getLogger(__name__)

db_module = get_db_module()


class IndexManager(QObject):
    """ç´¢å¼•ç®¡ç†å™¨ - ç®¡ç†æ–‡ä»¶ç´¢å¼•æ•°æ®åº“"""

    progress_signal = Signal(int, str)
    build_finished_signal = Signal()
    content_progress_signal = Signal(int, int, int, str)
    content_build_finished_signal = Signal(bool)
    fts_finished_signal = Signal()

    def __init__(self, db_path=None, config_mgr=None):
        super().__init__()
        self.config_mgr = config_mgr
        if db_path is None:
            idx_dir = LOG_DIR
            idx_dir.mkdir(exist_ok=True)
            self.db_path = str(idx_dir / "index.db")
        else:
            self.db_path = db_path

        self.conn = None
        self.lock = threading.RLock()
        self.is_ready = False
        self.is_building = False
        # flag to support cancelling content index build
        self._stop_content_build = False
        self.file_count = 0
        self.last_build_time = None
        self.last_build_duration = None
        self.has_fts = False
        self.used_mft = False

        # in-memory trigram index (prototype)
        try:
            self.trigram_index = TrigramIndex()
        except Exception:
            self.trigram_index = None

        self._init_db()

    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        try:
            if HAS_APSW:
                self.conn = db_module.Connection(self.db_path)
            else:
                self.conn = db_module.connect(self.db_path, check_same_thread=False)

            cursor = self.conn.cursor()
            cursor.execute("PRAGMA journal_mode=WAL")
            cursor.execute("PRAGMA synchronous=NORMAL")
            cursor.execute("PRAGMA cache_size=-2000000")
            cursor.execute("PRAGMA temp_store=MEMORY")

            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS files (
                    id INTEGER PRIMARY KEY,
                    filename TEXT NOT NULL,
                    filename_lower TEXT NOT NULL,
                    full_path TEXT UNIQUE NOT NULL,
                    parent_dir TEXT NOT NULL,
                    extension TEXT,
                    size INTEGER DEFAULT 0,
                    mtime REAL DEFAULT 0,
                    is_dir INTEGER DEFAULT 0
                )
            """
            )

            try:
                fts_exists = False
                for row in cursor.execute(
                    "SELECT name FROM sqlite_master WHERE type='table' AND name='files_fts'"
                ):
                    fts_exists = True
                    break
                if not fts_exists:
                    cursor.execute(
                        "CREATE VIRTUAL TABLE files_fts USING fts5(filename, content=files, content_rowid=id)"
                    )
                    cursor.execute(
                        """
                        CREATE TRIGGER IF NOT EXISTS files_ai AFTER INSERT ON files BEGIN
                            INSERT INTO files_fts(rowid, filename) VALUES (new.id, new.filename);
                        END
                    """
                    )
                    cursor.execute(
                        """
                        CREATE TRIGGER IF NOT EXISTS files_ad AFTER DELETE ON files BEGIN
                            INSERT INTO files_fts(files_fts, rowid, filename) VALUES('delete', old.id, old.filename);
                        END
                    """
                    )
                # å°è¯•å¡«å…… FTS5 å†…å®¹è¡¨ï¼ˆå¦‚æœå°šæœªå¡«å……ï¼‰å¹¶æ£€æŸ¥è¡Œæ•°ä¸€è‡´æ€§
                try:
                    cursor.execute("INSERT INTO files_fts(rowid, filename) SELECT id, filename FROM files")
                    if not HAS_APSW:
                        self.conn.commit()
                except Exception:
                    # æŸäº› SQLite/FTS ç¯å¢ƒä¸‹ insert å¯èƒ½å¤±è´¥ï¼›ç»§ç»­ä½†è®°å½•æ—¥å¿—
                    logger.debug("å°è¯•å¡«å…… files_fts å¤±è´¥ï¼ˆå¯èƒ½å·²å­˜åœ¨æˆ–ä¸æ”¯æŒ INSERT INTO ftsï¼‰ï¼Œå°†è·³è¿‡å¡«å……")
                try:
                    files_cnt = list(cursor.execute("SELECT COUNT(*) FROM files"))[0][0]
                    fts_cnt = list(cursor.execute("SELECT COUNT(*) FROM files_fts"))[0][0]
                    if files_cnt != fts_cnt:
                        logger.warning(f"FTS5 è¡Œæ•°ä¸ä¸»è¡¨ä¸ä¸€è‡´: files={files_cnt}, files_fts={fts_cnt}")
                except Exception:
                    logger.debug("æ— æ³•æ¯”è¾ƒ files ä¸ files_fts çš„è¡Œæ•°")
                self.has_fts = True
                logger.info("âœ… FTS5 å·²å¯ç”¨")
                # å°è¯•åˆ›å»ºå†…å®¹ FTS è¡¨ï¼ˆç”¨äºæ–‡ä»¶å†…å®¹å…¨æ–‡æœç´¢ï¼‰
                try:
                    content_exists = False
                    for row in cursor.execute(
                        "SELECT name FROM sqlite_master WHERE type='table' AND name='content_fts'"
                    ):
                        content_exists = True
                        break
                    if not content_exists:
                        # content åˆ—ä¸ºå…¨æ–‡ç´¢å¼•ï¼Œpath ä¸ fileid ä¸å‚ä¸å€’æ’ç´¢å¼•
                        cursor.execute(
                            "CREATE VIRTUAL TABLE content_fts USING fts5(content, path UNINDEXED, fileid UNINDEXED)"
                        )
                    self.has_content_fts = True
                    logger.info("âœ… content FTS5 è¡¨å·²åˆ›å»ºï¼ˆç”¨äºæ–‡ä»¶å†…å®¹æœç´¢ï¼‰")
                except Exception as e:
                    self.has_content_fts = False
                    logger.warning(f"âš ï¸ content FTS5 ä¸å¯ç”¨: {e}")
            except Exception as e:
                self.has_fts = False
                logger.warning(f"âš ï¸ FTS5 ä¸å¯ç”¨: {e}")

            cursor.execute("CREATE INDEX IF NOT EXISTS idx_fn ON files(filename_lower)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_parent ON files(parent_dir)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_ext ON files(extension)")
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS meta (key TEXT PRIMARY KEY, value TEXT)"
            )

            if not HAS_APSW:
                self.conn.commit()

            self._load_stats()
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–é”™è¯¯: {e}")
            self.conn = None
            self.is_ready = False

    def _load_stats(self, preserve_mft=False):
        """åŠ è½½ç»Ÿè®¡ä¿¡æ¯"""
        if not self.conn:
            return
        try:
            with self.lock:
                cursor = self.conn.cursor()

                count_result = list(cursor.execute("SELECT COUNT(*) FROM files"))
                self.file_count = count_result[0][0] if count_result else 0

                time_row = list(cursor.execute("SELECT value FROM meta WHERE key='build_time'"))
                if time_row and time_row[0][0]:
                    try:
                        self.last_build_time = float(time_row[0][0])
                    except (ValueError, TypeError):
                        self.last_build_time = None
                else:
                    self.last_build_time = None

                dur_row = list(cursor.execute("SELECT value FROM meta WHERE key='build_duration'"))
                if dur_row and dur_row[0][0]:
                    try:
                        self.last_build_duration = float(dur_row[0][0])
                    except (ValueError, TypeError):
                        self.last_build_duration = None
                else:
                    self.last_build_duration = None

                if self.last_build_time in (None, 0):
                    # å›é€€ï¼šè‹¥æœªå†™å…¥ metaï¼Œåˆ™ä½¿ç”¨ç´¢å¼•åº“æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
                    try:
                        if os.path.exists(self.db_path):
                            self.last_build_time = os.path.getmtime(self.db_path)
                    except Exception:
                        pass

                if not preserve_mft:
                    mft_row = list(cursor.execute("SELECT value FROM meta WHERE key='used_mft'"))
                    self.used_mft = bool(mft_row and mft_row[0][0] == "1")

            self.is_ready = self.file_count > 0
        except Exception as e:
            logger.error(f"åŠ è½½ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            self.file_count = 0
            self.is_ready = False

    def reload_stats(self):
        if not self.is_building:
            self._load_stats(preserve_mft=True)

    def force_reload_stats(self):
        self._load_stats(preserve_mft=True)

    def close(self):
        with self.lock:
            if self.conn:
                try:
                    self.conn.close()
                    logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")
                except Exception as e:
                    logger.warning(f"å…³é—­æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
                finally:
                    self.conn = None

    def search(self, keywords, scope_targets, limit=50000):
        if not self.conn or not self.is_ready:
            logger.warning("æœç´¢è¯·æ±‚æ—¶ç´¢å¼•ä¸å¯ç”¨æˆ–æœªå‡†å¤‡å¥½: conn=%s, is_ready=%s", bool(self.conn), self.is_ready)
            # é™é»˜å›é€€ï¼šä¸æŠ›é”™ï¼Œè¿”å›ç©ºç»“æœï¼Œé¿å…æ‰“æ–­æœç´¢æµç¨‹
            return []

        try:
            with self.lock:
                cursor = self.conn.cursor()
                keyword_str = ' '.join(keywords) if isinstance(keywords, list) else keywords
                parsed_keywords, filters, or_keywords, not_keywords = self._parse_search_syntax(keyword_str)
                
                # è¯¦ç»†è°ƒè¯•æ—¥å¿—
                logger.info(f"ğŸ” æœç´¢å‚æ•° - åŸå§‹å…³é”®è¯: {keywords}, å…³é”®è¯å­—ç¬¦ä¸²: {keyword_str}")
                logger.info(f"ğŸ” è§£æç»“æœ - ANDå…³é”®è¯: {parsed_keywords}, ORå…³é”®è¯: {or_keywords}, NOTå…³é”®è¯: {not_keywords}, è¿‡æ»¤å™¨: {filters}")

                if not parsed_keywords and not or_keywords and not any([
                    filters['ext'], filters['ext_list'], filters['size_min'], filters['size_max'],
                    filters['dm_after'], filters['dm_before'], filters['type'], filters['path'],
                    filters['len_min'], filters['len_max']
                ]):
                    return []

                # If the configuration requests a simple 'Everything'-like
                # substring search mode, use a straightforward SQL query that
                # matches keywords against both filename and full_path. This
                # keeps behavior simple and predictable for users who prefer
                # Everything-style searching. Otherwise, fall back to using
                # the in-memory trigram index when available for candidate
                # selection, and finally the SQL LIKE path.
                simple_mode = True
                try:
                    if getattr(self, 'config_mgr', None) is not None:
                        simple_mode = bool(self.config_mgr.get_search_simple_mode())
                except Exception:
                    simple_mode = True

                if simple_mode:
                    # Everything é£æ ¼ç®€å•æ¨¡å¼ï¼šåŒ¹é…æ–‡ä»¶åæˆ–è·¯å¾„
                    match_on_path = True
                    conditions = []
                    params = []

                    # é€šé…ç¬¦è½¬æ¢è¾…åŠ©å‡½æ•°
                    def wildcard_to_sql(pattern):
                        """å°† Everything é£æ ¼é€šé…ç¬¦è½¬æ¢ä¸º SQL LIKE æ¨¡å¼
                        * -> %
                        ? -> _
                        """
                        # è½¬ä¹‰ SQL LIKE ç‰¹æ®Šå­—ç¬¦ï¼ˆé™¤äº†å³å°†æ›¿æ¢çš„ * å’Œ ?ï¼‰
                        pattern = pattern.replace('[', r'\[').replace('%', r'\%').replace('_', r'\_')
                        # è½¬æ¢é€šé…ç¬¦
                        pattern = pattern.replace('*', '%').replace('?', '_')
                        return pattern

                    # AND å…³é”®è¯ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰
                    for kw in parsed_keywords:
                        sql_pattern = wildcard_to_sql(kw)
                        conditions.append("(filename_lower LIKE ? ESCAPE '\\' OR lower(full_path) LIKE ? ESCAPE '\\')")
                        params.append(f"%{sql_pattern}%")
                        params.append(f"%{sql_pattern}%")

                    # OR å…³é”®è¯ï¼ˆ| åˆ†éš”ï¼‰
                    if or_keywords:
                        or_conditions = []
                        for or_kw in or_keywords:
                            sql_pattern = wildcard_to_sql(or_kw)
                            or_conditions.append("(filename_lower LIKE ? ESCAPE '\\' OR lower(full_path) LIKE ? ESCAPE '\\')")
                            params.append(f"%{sql_pattern}%")
                            params.append(f"%{sql_pattern}%")
                        if or_conditions:
                            conditions.append(f"({' OR '.join(or_conditions)})")

                    # NOT å…³é”®è¯ï¼ˆ! å‰ç¼€ï¼‰
                    for not_kw in not_keywords:
                        sql_pattern = wildcard_to_sql(not_kw)
                        conditions.append("NOT (filename_lower LIKE ? ESCAPE '\\' OR lower(full_path) LIKE ? ESCAPE '\\')")
                        params.append(f"%{sql_pattern}%")
                        params.append(f"%{sql_pattern}%")

                    # æ‰©å±•åè¿‡æ»¤ï¼ˆæ”¯æŒå¤šä¸ªæ‰©å±•å ORï¼‰
                    if filters['ext_list']:
                        ext_conditions = []
                        for ext in filters['ext_list']:
                            ext_conditions.append("extension = ?")
                            params.append(ext)
                        conditions.append(f"({' OR '.join(ext_conditions)})")
                    elif filters['ext']:
                        conditions.append("extension = ?")
                        params.append(filters['ext'])

                    # æ–‡ä»¶ç±»å‹è¿‡æ»¤
                    if filters['type'] == 'folder':
                        conditions.append("is_dir = 1")
                    elif filters['type'] == 'file':
                        conditions.append("is_dir = 0")

                    # å¤§å°è¿‡æ»¤
                    if filters['size_min'] > 0:
                        conditions.append("size > ?")
                        params.append(filters['size_min'])
                    if filters['size_max'] > 0:
                        conditions.append("size < ?")
                        params.append(filters['size_max'])

                    # ä¿®æ”¹æ—¶é—´è¿‡æ»¤
                    if filters['dm_after'] > 0:
                        conditions.append("(mtime >= ? OR mtime = 0)")
                        params.append(filters['dm_after'])
                    if filters['dm_before'] > 0:
                        conditions.append("(mtime <= ? OR mtime = 0)")
                        params.append(filters['dm_before'])

                    # è·¯å¾„é•¿åº¦è¿‡æ»¤
                    if filters['len_min'] > 0:
                        conditions.append("LENGTH(full_path) > ?")
                        params.append(filters['len_min'])
                    if filters['len_max'] > 0:
                        conditions.append("LENGTH(full_path) < ?")
                        params.append(filters['len_max'])

                    # è·¯å¾„åŒ…å«è¿‡æ»¤
                    if filters['path']:
                        conditions.append("lower(full_path) LIKE ?")
                        params.append(f"%{filters['path']}%")

                    # å†…å®¹æœç´¢ï¼ˆä½¿ç”¨ content_fts FTS5 è¡¨ï¼‰
                    if filters.get('content'):
                        if getattr(self, 'has_content_fts', False):
                            conditions.append("EXISTS (SELECT 1 FROM content_fts c WHERE c.rowid = files.id AND c.content MATCH ?)")
                            params.append(filters['content'])
                        else:
                            # å¦‚æœ FTS5 å†…å®¹ç´¢å¼•ä¸å¯ç”¨ï¼Œè®°å½•å¹¶å¿½ç•¥ï¼ˆæˆ–å¯é€‰æ‹©å›é€€åˆ°æ…¢é€Ÿæ‰«æï¼‰
                            logger.warning("å†…å®¹æœç´¢è¢«è¯·æ±‚ä½† content_fts ä¸å¯ç”¨ï¼Œè·³è¿‡å†…å®¹è¿‡æ»¤")

                    where_clause = " AND ".join(conditions) if conditions else "1=1"
                    sql = f"""
                        SELECT filename, full_path, size, mtime, is_dir
                        FROM files
                        WHERE {where_clause}
                        LIMIT ?
                    """
                    params.append(limit)
                    
                    logger.info(f"ğŸ” SQLæŸ¥è¯¢ - simple_mode: {simple_mode}, match_on_path: {match_on_path}")
                    logger.info(f"ğŸ” SQL: {sql}")
                    logger.info(f"ğŸ” å‚æ•°: {params}")
                    
                    raw_results = list(cursor.execute(sql, tuple(params)))
                    logger.info(f"ğŸ” SQLè¿”å›åŸå§‹ç»“æœæ•°: {len(raw_results)}")
                else:
                    # é«˜çº§æ¨¡å¼ï¼šåªåŒ¹é…æ–‡ä»¶å
                    match_on_path = False
                    conditions = []
                    params = []

                    # é€šé…ç¬¦è½¬æ¢è¾…åŠ©å‡½æ•°
                    def wildcard_to_sql(pattern):
                        """å°† Everything é£æ ¼é€šé…ç¬¦è½¬æ¢ä¸º SQL LIKE æ¨¡å¼"""
                        pattern = pattern.replace('[', r'\[').replace('%', r'\%').replace('_', r'\_')
                        pattern = pattern.replace('*', '%').replace('?', '_')
                        return pattern

                    # AND å…³é”®è¯
                    for kw in parsed_keywords:
                        sql_pattern = wildcard_to_sql(kw)
                        conditions.append("filename_lower LIKE ? ESCAPE '\\'")
                        params.append(f"%{sql_pattern}%")

                    # OR å…³é”®è¯
                    if or_keywords:
                        or_conditions = []
                        for or_kw in or_keywords:
                            sql_pattern = wildcard_to_sql(or_kw)
                            or_conditions.append("filename_lower LIKE ? ESCAPE '\\'")
                            params.append(f"%{sql_pattern}%")
                        if or_conditions:
                            conditions.append(f"({' OR '.join(or_conditions)})")

                    # NOT å…³é”®è¯
                    for not_kw in not_keywords:
                        sql_pattern = wildcard_to_sql(not_kw)
                        conditions.append("NOT filename_lower LIKE ? ESCAPE '\\'")
                        params.append(f"%{sql_pattern}%")

                    # æ‰©å±•åè¿‡æ»¤ï¼ˆæ”¯æŒå¤šä¸ªæ‰©å±•å ORï¼‰
                    if filters['ext_list']:
                        ext_conditions = []
                        for ext in filters['ext_list']:
                            ext_conditions.append("extension = ?")
                            params.append(ext)
                        conditions.append(f"({' OR '.join(ext_conditions)})")
                    elif filters['ext']:
                        conditions.append("extension = ?")
                        params.append(filters['ext'])

                    # æ–‡ä»¶ç±»å‹è¿‡æ»¤
                    if filters['type'] == 'folder':
                        conditions.append("is_dir = 1")
                    elif filters['type'] == 'file':
                        conditions.append("is_dir = 0")

                    # å¤§å°è¿‡æ»¤
                    if filters['size_min'] > 0:
                        conditions.append("size > ?")
                        params.append(filters['size_min'])
                    if filters['size_max'] > 0:
                        conditions.append("size < ?")
                        params.append(filters['size_max'])

                    # ä¿®æ”¹æ—¶é—´è¿‡æ»¤
                    if filters['dm_after'] > 0:
                        conditions.append("(mtime >= ? OR mtime = 0)")
                        params.append(filters['dm_after'])
                    if filters['dm_before'] > 0:
                        conditions.append("(mtime <= ? OR mtime = 0)")
                        params.append(filters['dm_before'])

                    # è·¯å¾„é•¿åº¦è¿‡æ»¤
                    if filters['len_min'] > 0:
                        conditions.append("LENGTH(full_path) > ?")
                        params.append(filters['len_min'])
                    if filters['len_max'] > 0:
                        conditions.append("LENGTH(full_path) < ?")
                        params.append(filters['len_max'])

                    # è·¯å¾„åŒ…å«è¿‡æ»¤
                    if filters['path']:
                        conditions.append("lower(full_path) LIKE ?")
                        params.append(f"%{filters['path']}%")

                    # å†…å®¹æœç´¢ï¼ˆä½¿ç”¨ content_fts FTS5 è¡¨ï¼‰
                    if filters.get('content'):
                        if getattr(self, 'has_content_fts', False):
                            conditions.append("EXISTS (SELECT 1 FROM content_fts c WHERE c.rowid = files.id AND c.content MATCH ?)")
                            params.append(filters['content'])
                        else:
                            logger.warning("å†…å®¹æœç´¢è¢«è¯·æ±‚ä½† content_fts ä¸å¯ç”¨ï¼Œè·³è¿‡å†…å®¹è¿‡æ»¤")

                    where_clause = " AND ".join(conditions) if conditions else "1=1"
                    sql = f"""
                        SELECT filename, full_path, size, mtime, is_dir
                        FROM files
                        WHERE {where_clause}
                        LIMIT ?
                    """
                    params.append(limit)
                    
                    logger.info(f"ğŸ” SQLæŸ¥è¯¢ - simple_mode: {simple_mode}, match_on_path: {match_on_path}")
                    logger.info(f"ğŸ” SQL: {sql}")
                    logger.info(f"ğŸ” å‚æ•°: {params}")
                    
                    raw_results = list(cursor.execute(sql, tuple(params)))
                    logger.info(f"ğŸ” SQLè¿”å›åŸå§‹ç»“æœæ•°: {len(raw_results)}")
                
                # Apply scope filtering and path/dir skip logic
                scope_drives = set()
                scope_paths = []
                if scope_targets:
                    for t in scope_targets:
                        t_norm = os.path.normpath(t).lower().rstrip("\\")
                        drv = Path(t_norm).drive.lower()
                        if drv and (t_norm == drv or t_norm == drv + "\\"):
                            scope_drives.add(drv)
                        else:
                            scope_paths.append(t_norm)

                filtered = []
                for fn, fp, sz, mt, is_dir in raw_results:
                    path_norm = os.path.normpath(fp)
                    path_lower = path_norm.lower()
                    path_drive = Path(path_lower).drive.lower()

                    if scope_targets:
                        ok = False
                        if scope_drives and path_drive in scope_drives:
                            ok = True
                        if not ok and scope_paths:
                            for p in scope_paths:
                                if path_lower == p or path_lower.startswith(p + "\\"):
                                    ok = True
                                    break
                        if not ok:
                            continue

                    if filters['path'] and filters['path'] not in path_lower:
                        continue

                    if should_skip_path(path_lower):
                        continue

                    name_lower = fn.lower()
                    if is_dir:
                        if should_skip_dir(name_lower, path_lower):
                            continue
                    else:
                        if os.path.splitext(name_lower)[1] in SKIP_EXTS:
                            continue

                    filtered.append((fn, fp, sz, mt, is_dir))

            logger.info(f"ğŸ” èŒƒå›´/è·¯å¾„è¿‡æ»¤åç»“æœæ•°: {len(filtered)}")
            
            if filtered and filters.get('dm_after', 0) > 0:
                needs_fix_count = sum(1 for item in filtered if item[3] == 0)
                if needs_fix_count > 0:
                    logger.info(f"âš ï¸ å‘ç° {needs_fix_count} ä¸ªæ–‡ä»¶ mtime=0ï¼Œä½¿ç”¨å¿«é€Ÿè¡¥é½ï¼ˆå¤šçº¿ç¨‹ï¼‰...")
                    start_time = time.time()
                    max_fix = 10000
                    needs_fix = [(i, item[1]) for i, item in enumerate(filtered) if item[3] == 0][:max_fix]

                    def get_mtime_fast(idx_path):
                        idx, fpath = idx_path
                        try:
                            return (idx, os.stat(fpath).st_mtime, fpath)
                        except Exception:
                            return (idx, 0, fpath)

                    fixed_items = {}
                    db_updates = []

                    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
                        for idx, mt, fpath in executor.map(get_mtime_fast, needs_fix):
                            fixed_items[idx] = mt
                            if mt > 0:
                                db_updates.append((mt, fpath))

                    new_filtered = []
                    for i, (fn, fp, sz, mt, is_dir) in enumerate(filtered):
                        if i in fixed_items:
                            mt = fixed_items[i]
                        if mt > 0 and mt >= filters['dm_after']:
                            new_filtered.append((fn, fp, sz, mt, is_dir))

                    filtered = new_filtered
                    elapsed = time.time() - start_time
                    logger.info(f"âœ… è¡¥é½å®Œæˆ: {len(needs_fix)} ä¸ªæ–‡ä»¶ï¼Œè€—æ—¶ {elapsed:.2f}sï¼Œå‰©ä½™ {len(filtered)} ä¸ª")

                    if db_updates:
                        def update_db():
                            try:
                                with self.lock:
                                    cursor = self.conn.cursor()
                                    cursor.executemany("UPDATE files SET mtime=? WHERE full_path=?", db_updates)
                                    if not HAS_APSW:
                                        self.conn.commit()
                                logger.info(f"ğŸ“ å·²ç¼“å­˜ {len(db_updates)} ä¸ªæ–‡ä»¶çš„ mtime åˆ°æ•°æ®åº“")
                            except Exception as e:
                                logger.debug(f"æ•°æ®åº“æ›´æ–°å¤±è´¥: {e}")

                        threading.Thread(target=update_db, daemon=True).start()

            logger.info(f"âœ… æœç´¢å®Œæˆï¼Œæœ€ç»ˆè¿”å›ç»“æœæ•°: {len(filtered)}")
            return filtered

        except Exception as e:
            logger.error(f"æœç´¢é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            # é™é»˜å›é€€ï¼šå¼‚å¸¸æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…å‰ç«¯å¼¹çª—
            return []

    def _search_like(self, cursor, keywords, limit):
        wheres = " AND ".join(["filename_lower LIKE ?"] * len(keywords))
        sql = f"""
            SELECT filename, full_path, size, mtime, is_dir
            FROM files
            WHERE {wheres}
            LIMIT ?
        """
        params = tuple([f"%{kw}%" for kw in keywords] + [limit])
        return list(cursor.execute(sql, params))

    def get_stats(self):
        self._load_stats(preserve_mft=True)
        return {
            "count": self.file_count,
            "ready": self.is_ready,
            "building": self.is_building,
            "time": self.last_build_time,
            "duration": self.last_build_duration,
            "path": self.db_path,
            "has_fts": self.has_fts,
            "used_mft": self.used_mft,
        }

    def build_index(self, drives, stop_fn=None):
        from . import mft_scanner  # avoid circular

        if not self.conn or self.is_building:
            return

        self.is_building = True
        self.is_ready = False
        self.used_mft = False
        mft_scanner.MFT_AVAILABLE = False
        build_start = time.time()

        try:
            logger.info("ğŸš€ å¼€å§‹æ„å»ºç´¢å¼•...")
            self.progress_signal.emit(0, "é˜¶æ®µ1/5: æ¸…ç†æ—§æ•°æ®...")

            with self.lock:
                cursor = self.conn.cursor()
                cursor.execute("DROP TRIGGER IF EXISTS files_ai")
                cursor.execute("DROP TRIGGER IF EXISTS files_ad")
                cursor.execute("DROP TABLE IF EXISTS files_fts")
                cursor.execute("DROP TABLE IF EXISTS files")
                cursor.execute(
                    """
                    CREATE TABLE files (
                        id INTEGER PRIMARY KEY,
                        filename TEXT NOT NULL,
                        filename_lower TEXT NOT NULL,
                        full_path TEXT UNIQUE NOT NULL,
                        parent_dir TEXT NOT NULL,
                        extension TEXT,
                        size INTEGER DEFAULT 0,
                        mtime REAL DEFAULT 0,
                        is_dir INTEGER DEFAULT 0
                    )
                """
                )
                if not HAS_APSW:
                    self.conn.commit()
                self.has_fts = False
                self.file_count = 0

            logger.info(f"âœ… é˜¶æ®µ1å®Œæˆ: {time.time() - build_start:.2f}s")

            self.progress_signal.emit(0, "é˜¶æ®µ2/5: MFTæ‰«æ...")
            all_drives = [d.upper().rstrip(":\\") for d in drives if os.path.exists(d)]
            c_allowed_paths = get_c_scan_dirs(self.config_mgr)
            all_data = []
            failed_drives = []

            if all_drives and IS_WINDOWS:
                data_lock = threading.Lock()

                def scan_one(drv):
                    try:
                        allowed = c_allowed_paths if drv == "C" else None
                        data = enum_volume_files_mft(
                            drv, SKIP_DIRS_LOWER, SKIP_EXTS, allowed_paths=allowed
                        )
                        with data_lock:
                            all_data.extend(data)
                        return drv, len(data)
                    except Exception as e:
                        logger.error(f"æ‰«æé©±åŠ¨å™¨ {drv} å¤±è´¥: {e}")
                        return drv, -1

                with concurrent.futures.ThreadPoolExecutor(
                    max_workers=min(len(all_drives), 4)
                ) as ex:
                    futures = [ex.submit(scan_one, d) for d in all_drives]
                    for future in concurrent.futures.as_completed(futures):
                        if stop_fn and stop_fn():
                            break
                        drv, result = future.result()
                        if result < 0:
                            failed_drives.append(drv)
                        self.progress_signal.emit(
                            len(all_data),
                            f"MFT {drv}: {result if result >= 0 else 'å¤±è´¥'}",
                        )

                if all_data:
                    self.used_mft = True

            logger.info(
                f"âœ… é˜¶æ®µ2å®Œæˆ: {time.time() - build_start:.2f}s, æ‰«æåˆ° {len(all_data):,} æ¡"
            )

            if all_data:
                self.progress_signal.emit(len(all_data), "é˜¶æ®µ3/5: å†™å…¥æ•°æ®åº“...")
                write_start = time.time()

                with self.lock:
                    cursor = self.conn.cursor()
                    cursor.execute("PRAGMA synchronous=OFF")
                    cursor.execute("PRAGMA journal_mode=MEMORY")
                    cursor.execute("PRAGMA locking_mode=EXCLUSIVE")
                    cursor.execute("PRAGMA temp_store=MEMORY")
                    cursor.execute("PRAGMA cache_size=-500000")
                    cursor.execute("PRAGMA mmap_size=268435456")

                    if HAS_APSW:
                        with self.conn:
                            cursor.executemany(
                                "INSERT OR IGNORE INTO files VALUES(NULL,?,?,?,?,?,?,?,?)",
                                all_data
                            )
                    else:
                        cursor.executemany(
                            "INSERT OR IGNORE INTO files VALUES(NULL,?,?,?,?,?,?,?,?)",
                            all_data
                        )
                        self.conn.commit()

                self.file_count = len(all_data)
                write_time = time.time() - write_start
                logger.info(f"âœ… é˜¶æ®µ3å®Œæˆ: {write_time:.2f}s, å†™å…¥ {len(all_data):,} æ¡")

                # build in-memory trigram index from all_data for fast candidate selection
                try:
                    if getattr(self, 'trigram_index', None) is not None:
                        docs = []
                        for fn, fn_lower, fp, cur, ext, sz, mt, is_dir in all_data:
                            docs.append({
                                'filename': fn,
                                'dir_path': cur,
                                'fullpath': fp,
                                'size': sz,
                                'mtime': mt,
                                'type_code': is_dir,
                            })
                        # build index (in-memory)
                        self.trigram_index.build_index(docs)
                except Exception:
                    pass

            self.progress_signal.emit(self.file_count, "é˜¶æ®µ4/5: åˆ›å»ºç´¢å¼•...")
            with self.lock:
                cursor = self.conn.cursor()
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_fn ON files(filename_lower)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_parent ON files(parent_dir)")
                cursor.execute("PRAGMA synchronous=NORMAL")
                cursor.execute("PRAGMA journal_mode=WAL")
                now_ts = time.time()
                cursor.execute(
                    "INSERT OR REPLACE INTO meta (key, value) VALUES ('build_time', ?)",
                    (str(now_ts),),
                )
                cursor.execute(
                    "INSERT OR REPLACE INTO meta (key, value) VALUES ('build_duration', ?)",
                    (str(time.time() - build_start),),
                )
                cursor.execute(
                    "INSERT OR REPLACE INTO meta (key, value) VALUES ('used_mft', ?)",
                    ("1" if self.used_mft else "0",),
                )
                if not HAS_APSW:
                    self.conn.commit()

            logger.info(f"âœ… é˜¶æ®µ4å®Œæˆ: {time.time() - build_start:.2f}s")

            self.progress_signal.emit(self.file_count, "é˜¶æ®µ5/5: æ„å»ºå…¨æ–‡ç´¢å¼•(åå°)...")

            def build_fts_async():
                try:
                    logger.info("ğŸ“ åå°æ„å»º FTS5...")
                    fts_start = time.time()
                    with self.lock:
                        cursor = self.conn.cursor()
                        cursor.execute(
                            "CREATE VIRTUAL TABLE IF NOT EXISTS files_fts USING fts5(filename, content=files, content_rowid=id)"
                        )
                        cursor.execute("INSERT INTO files_fts(files_fts) VALUES('rebuild')")
                        cursor.execute(
                            """
                            CREATE TRIGGER IF NOT EXISTS files_ai AFTER INSERT ON files BEGIN
                                INSERT INTO files_fts(rowid, filename) VALUES (new.id, new.filename);
                            END
                        """
                        )
                        cursor.execute(
                            """
                            CREATE TRIGGER IF NOT EXISTS files_ad AFTER DELETE ON files BEGIN
                                INSERT INTO files_fts(files_fts, rowid, filename) VALUES('delete', old.id, old.filename);
                            END
                        """
                        )
                        if not HAS_APSW:
                            self.conn.commit()
                        self.has_fts = True
                    logger.info(f"âœ… FTS5 æ„å»ºå®Œæˆ: {time.time() - fts_start:.2f}s")
                except Exception as e:
                    logger.warning(f"âš ï¸ FTS5 æ„å»ºå¤±è´¥: {e}")
                    self.has_fts = False
                self.fts_finished_signal.emit()

            threading.Thread(target=build_fts_async, daemon=True).start()

            for drv in failed_drives:
                if stop_fn and stop_fn():
                    break
                paths_to_scan = c_allowed_paths if drv == "C" else [f"{drv}:\\"]
                for path in paths_to_scan:
                    logger.info(f"[ä¼ ç»Ÿæ‰«æ] {path}")
                    self._scan_dir(path, c_allowed_paths if drv == "C" else None, stop_fn)

            try:
                with self.lock:
                    cursor = self.conn.cursor()
                    final_count = list(cursor.execute("SELECT COUNT(*) FROM files"))[0][0]
                    self.file_count = final_count
            except Exception:
                pass

            total_time = time.time() - build_start
            logger.info(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {self.file_count:,} æ¡, æ€»è€—æ—¶ {total_time:.2f}s")
            self.is_ready = self.file_count > 0
            self.build_finished_signal.emit()

        except Exception as e:
            import traceback
            logger.error(f"âŒ æ„å»ºé”™è¯¯: {e}")
            traceback.print_exc()
        finally:
            self.is_building = False

    def _scan_dir(self, target, allowed_paths=None, stop_fn=None):
        try:
            if not os.path.exists(target):
                return
        except (OSError, PermissionError):
            logger.warning(f"æ— æ³•è®¿é—®ç›®å½•: {target}")
            return

        allowed_paths_lower = (
            [p.lower().rstrip("\\") for p in allowed_paths] if allowed_paths else None
        )
        batch = []
        stack = deque([target])

        while stack:
            if stop_fn and stop_fn():
                break
            cur = stack.pop()
            if should_skip_path(cur.lower(), allowed_paths_lower):
                continue
            try:
                with os.scandir(cur) as it:
                    for e in it:
                        if stop_fn and stop_fn():
                            break
                        if not e.name or e.name.startswith((".", "$")):
                            continue
                        try:
                            is_dir = e.is_dir()
                            st = e.stat(follow_symlinks=False)
                        except (OSError, PermissionError):
                            continue

                        path_lower = e.path.lower()
                        if is_dir:
                            if should_skip_dir(e.name.lower(), path_lower, allowed_paths_lower):
                                continue
                            stack.append(e.path)
                            batch.append((e.name, e.name.lower(), e.path, cur, "", 0, st.st_mtime, 1))
                        else:
                            ext = os.path.splitext(e.name)[1].lower()
                            if ext in SKIP_EXTS:
                                continue
                            batch.append((e.name, e.name.lower(), e.path, cur, ext, st.st_size, st.st_mtime, 0))

                        if len(batch) >= 20000:
                            with self.lock:
                                cursor = self.conn.cursor()
                                cursor.executemany(
                                    "INSERT OR IGNORE INTO files VALUES(NULL,?,?,?,?,?,?,?,?)",
                                    batch,
                                )
                                if not HAS_APSW:
                                    self.conn.commit()
                                # best-effort: add to in-memory trigram index
                                try:
                                    if getattr(self, 'trigram_index', None) is not None:
                                        for fn, fn_lower, fp, curp, ext, sz, mt, is_dir in batch:
                                            doc = {
                                                'filename': fn,
                                                'dir_path': curp,
                                                'fullpath': fp,
                                                'size': sz,
                                                'mtime': mt,
                                                'type_code': is_dir,
                                            }
                                            try:
                                                self.trigram_index.add_doc(doc)
                                            except Exception:
                                                pass
                                except Exception:
                                    pass
                            self.file_count += len(batch)
                            self.progress_signal.emit(self.file_count, cur)
                            batch = []
            except (PermissionError, OSError):
                continue

        if batch:
            with self.lock:
                cursor = self.conn.cursor()
                cursor.executemany(
                    "INSERT OR IGNORE INTO files VALUES(NULL,?,?,?,?,?,?,?,?)", batch
                )
                if not HAS_APSW:
                    self.conn.commit()
                # best-effort: add to in-memory trigram index
                try:
                    if getattr(self, 'trigram_index', None) is not None:
                        for fn, fn_lower, fp, curp, ext, sz, mt, is_dir in batch:
                            doc = {
                                'filename': fn,
                                'dir_path': curp,
                                'fullpath': fp,
                                'size': sz,
                                'mtime': mt,
                                'type_code': is_dir,
                            }
                            try:
                                self.trigram_index.add_doc(doc)
                            except Exception:
                                pass
                except Exception:
                    pass
            self.file_count += len(batch)

    def rebuild_drive(self, drive_letter, progress_callback=None, stop_fn=None):
        if not self.conn:
            return
        if self.is_building:
            logger.warning("ç´¢å¼•æ­£åœ¨æ„å»ºä¸­ï¼Œè·³è¿‡")
            return

        self.is_building = True
        drive = drive_letter.upper().rstrip(":\\")
        try:
            logger.info(f"å¼€å§‹é‡å»º {drive}: ç›˜ç´¢å¼•...")
            with self.lock:
                cursor = self.conn.cursor()
                cursor.execute("DELETE FROM files WHERE full_path LIKE ?", (f"{drive}:%",))
                if not HAS_APSW:
                    self.conn.commit()

            c_allowed_paths = get_c_scan_dirs(self.config_mgr)
            allowed_paths = c_allowed_paths if drive == 'C' else None

            try:
                data = enum_volume_files_mft(drive, SKIP_DIRS_LOWER, SKIP_EXTS, allowed_paths)
                if data:
                    logger.info(f"å¼€å§‹å†™å…¥ {len(data)} æ¡è®°å½•...")
                    write_start = time.time()
                    with self.lock:
                        cursor = self.conn.cursor()
                        cursor.execute("PRAGMA synchronous=OFF")
                        cursor.execute("PRAGMA journal_mode=OFF")
                        cursor.execute("PRAGMA locking_mode=EXCLUSIVE")
                        cursor.execute("PRAGMA temp_store=MEMORY")
                        cursor.execute("PRAGMA cache_size=-500000")

                        if HAS_APSW:
                            with self.conn:
                                cursor.executemany(
                                    "INSERT OR IGNORE INTO files VALUES(NULL,?,?,?,?,?,?,?,?)",
                                    data
                                )
                                now_ts = time.time()
                                cursor.execute(
                                    "INSERT OR REPLACE INTO meta (key, value) VALUES ('build_time', ?)",
                                    (str(now_ts),)
                                )
                                cursor.execute(
                                    "INSERT OR REPLACE INTO meta (key, value) VALUES ('build_duration', ?)",
                                    (str(time.time() - build_start),)
                                )
                                cursor.execute(
                                    "INSERT OR REPLACE INTO meta (key, value) VALUES ('used_mft', '1')"
                                )
                        else:
                            cursor.execute("BEGIN TRANSACTION")
                            cursor.executemany(
                                "INSERT OR IGNORE INTO files VALUES(NULL,?,?,?,?,?,?,?,?)",
                                data
                            )
                            now_ts = time.time()
                            cursor.execute(
                                "INSERT OR REPLACE INTO meta (key, value) VALUES ('build_time', ?)",
                                (str(now_ts),)
                            )
                            cursor.execute(
                                "INSERT OR REPLACE INTO meta (key, value) VALUES ('build_duration', ?)",
                                (str(time.time() - build_start),)
                            )
                            cursor.execute(
                                "INSERT OR REPLACE INTO meta (key, value) VALUES ('used_mft', '1')"
                            )
                            cursor.execute("COMMIT")

                        cursor.execute("PRAGMA synchronous=NORMAL")
                        cursor.execute("PRAGMA journal_mode=WAL")
                        cursor.execute("PRAGMA locking_mode=NORMAL")

                    write_time = time.time() - write_start
                    logger.info(f"âœ… {drive}: ç›˜ç´¢å¼•é‡å»ºå®Œæˆï¼Œå†™å…¥ {len(data)} æ¡è®°å½•ï¼Œè€—æ—¶ {write_time:.2f}s")

            except Exception as e:
                logger.error(f"MFTæ‰«æå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

            self._load_stats(preserve_mft=True)
            self.is_ready = self.file_count > 0

        except Exception as e:
            logger.error(f"é‡å»ºé©±åŠ¨å™¨ {drive} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.is_building = False
            logger.info(f"{drive}: ç›˜ç´¢å¼•é‡å»ºæµç¨‹ç»“æŸ")
            self.build_finished_signal.emit()

    def _parse_search_syntax(self, keyword_str):
        """è§£æ Everything é£æ ¼çš„æœç´¢è¯­æ³•
        
        æ”¯æŒï¼š
        - é€šé…ç¬¦ï¼š* å’Œ ?
        - å¸ƒå°”è¿ç®—ï¼š| (OR), ! (NOT), ç©ºæ ¼ (AND)
        - è¿‡æ»¤å™¨ï¼šext:, size:, dm:, folder:, file:, path:, len:, attrib:, datemodified:
        """
        import re
        import datetime

        keywords = []
        or_keywords = []  # OR å…³é”®è¯åˆ—è¡¨
        not_keywords = []  # NOT å…³é”®è¯åˆ—è¡¨
        filters = {
            'ext': None,
            'ext_list': [],  # æ”¯æŒå¤šä¸ªæ‰©å±•åï¼ˆORï¼‰
            'size_min': 0,
            'size_max': 0,
            'dm_after': 0,
            'dm_before': 0,
            'type': None,
            'path': None,
            'content': None,
            'len_min': 0,
            'len_max': 0,
            'attrib_hidden': None,
            'attrib_readonly': None,
        }

        # é¢„å¤„ç†ï¼šå¤„ç† | åˆ†éš”çš„ OR è¯­æ³•
        # ä¾‹å¦‚ï¼šjpg|png|gif -> ä¼šè¢«æ‹†åˆ†ä¸ºå¤šä¸ª OR é€‰é¡¹
        def split_or_tokens(text):
            """åˆ†å‰² OR è¡¨è¾¾å¼ï¼Œæ”¯æŒ word1|word2|word3"""
            if '|' in text:
                return [t.strip() for t in text.split('|') if t.strip()]
            return [text]

        tokens = keyword_str.split()
        for token in tokens:
            token_lower = token.lower()
            
            # å¤„ç† NOT è¿ç®—ç¬¦ï¼ˆ!å…³é”®è¯ï¼‰
            if token.startswith('!') and len(token) > 1:
                not_term = token[1:].lower()
                # æ”¯æŒé€šé…ç¬¦è½¬æ¢ä¸ºæ­£åˆ™
                not_keywords.append(not_term)
                continue
            
            # æ‰©å±•åè¿‡æ»¤ï¼šext:jpg æˆ– ext:jpg|png|gif
            if token_lower.startswith('ext:'):
                ext_part = token[4:].strip()
                for ext in split_or_tokens(ext_part):
                    if ext and not ext.startswith('.'):
                        ext = '.' + ext
                    filters['ext_list'].append(ext.lower())
                if filters['ext_list']:
                    filters['ext'] = filters['ext_list'][0]  # å…¼å®¹æ—§é€»è¾‘
                continue
            
            # å¤§å°è¿‡æ»¤ï¼šsize:>1mb, size:<500kb, size:1mb..10mb
            if token_lower.startswith('size:'):
                size_part = token[5:].strip().lower()
                # èŒƒå›´è¯­æ³•ï¼šsize:1mb..10mb
                if '..' in size_part:
                    try:
                        min_str, max_str = size_part.split('..')
                        filters['size_min'] = self._parse_size(min_str)
                        filters['size_max'] = self._parse_size(max_str)
                    except:
                        pass
                else:
                    match = re.match(r'([<>])(\d+)(kb|mb|gb)?', size_part)
                    if match:
                        op, num, unit = match.groups()
                        num = int(num)
                        multiplier = {'kb': 1024, 'mb': 1024**2, 'gb': 1024**3}.get(unit, 1)
                        size_bytes = num * multiplier
                        if op == '>':
                            filters['size_min'] = size_bytes
                        else:
                            filters['size_max'] = size_bytes
                continue
            
            # ä¿®æ”¹æ—¶é—´è¿‡æ»¤ï¼šdm:today, dm:7d, dm:2024-12-01, dm:2024-12-01..2024-12-22
            if token_lower.startswith('dm:') or token_lower.startswith('datemodified:'):
                dm_part = token.split(':', 1)[1].strip().lower()
                now = time.time()
                day = 86400
                
                # èŒƒå›´è¯­æ³•ï¼šdm:2024-12-01..2024-12-22
                if '..' in dm_part:
                    try:
                        start_str, end_str = dm_part.split('..')
                        filters['dm_after'] = self._parse_date(start_str)
                        filters['dm_before'] = self._parse_date(end_str) + day  # åŒ…å«ç»“æŸæ—¥æœŸ
                    except:
                        pass
                elif dm_part == 'today':
                    today_start = datetime.datetime.now().replace(
                        hour=0, minute=0, second=0, microsecond=0
                    ).timestamp()
                    filters['dm_after'] = today_start
                elif dm_part.endswith('d') and dm_part[:-1].isdigit():
                    days = int(dm_part[:-1])
                    filters['dm_after'] = now - (days * day)
                elif dm_part.endswith('h') and dm_part[:-1].isdigit():
                    hours = int(dm_part[:-1])
                    filters['dm_after'] = now - (hours * 3600)
                elif re.match(r'\d{4}-\d{2}-\d{2}', dm_part):
                    # ç²¾ç¡®æ—¥æœŸï¼šdm:2024-12-22
                    try:
                        filters['dm_after'] = self._parse_date(dm_part)
                    except:
                        pass
                continue
            
            # è·¯å¾„é•¿åº¦è¿‡æ»¤ï¼šlen:>100, len:<50
            if token_lower.startswith('len:'):
                len_part = token[4:].strip()
                match = re.match(r'([<>])(\d+)', len_part)
                if match:
                    op, num = match.groups()
                    if op == '>':
                        filters['len_min'] = int(num)
                    else:
                        filters['len_max'] = int(num)
                continue
            
            # æ–‡ä»¶å±æ€§è¿‡æ»¤ï¼šattrib:h (hidden), attrib:r (readonly)
            if token_lower.startswith('attrib:'):
                attrib = token[7:].strip().lower()
                if 'h' in attrib:
                    filters['attrib_hidden'] = True
                if 'r' in attrib:
                    filters['attrib_readonly'] = True
                continue
            
            # æ–‡ä»¶å¤¹/æ–‡ä»¶ç±»å‹è¿‡æ»¤
            if token_lower.startswith('folder:'):
                filters['type'] = 'folder'
                rest = token[7:].strip()
                if rest:
                    keywords.append(rest.lower())
                continue
            
            if token_lower.startswith('file:'):
                filters['type'] = 'file'
                rest = token[5:].strip()
                if rest:
                    keywords.append(rest.lower())
                continue
            
            # è·¯å¾„åŒ…å«è¿‡æ»¤
            if token_lower.startswith('path:'):
                path_part = token[5:].strip()
                if path_part:
                    filters['path'] = path_part.lower()
                continue
            # æ–‡ä»¶å†…å®¹æœç´¢è¿‡æ»¤å™¨
            if token_lower.startswith('content:'):
                # æ”¯æŒ content:"phrase with spaces" æˆ– content:word
                content_part = token.split(':', 1)[1].strip()
                if content_part:
                    filters['content'] = content_part
                continue
            
            # å¤„ç† OR å…³é”®è¯ï¼ˆåŒ…å« | çš„ï¼‰
            if '|' in token:
                or_keywords.extend(split_or_tokens(token))
                continue
            
            # æ™®é€šå…³é”®è¯ï¼ˆæ”¯æŒé€šé…ç¬¦ * å’Œ ?ï¼‰
            keywords.append(token.lower())

        return keywords, filters, or_keywords, not_keywords
    
    def _parse_size(self, size_str):
        """è§£æå¤§å°å­—ç¬¦ä¸²ï¼š1mb, 500kb, 10gb"""
        import re
        match = re.match(r'(\d+)(kb|mb|gb)?', size_str.lower())
        if match:
            num, unit = match.groups()
            multiplier = {'kb': 1024, 'mb': 1024**2, 'gb': 1024**3}.get(unit, 1)
            return int(num) * multiplier
        return 0
    
    def _parse_date(self, date_str):
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼š2024-12-22"""
        import datetime
        dt = datetime.datetime.strptime(date_str, '%Y-%m-%d')
        return dt.timestamp()

    def _ensure_extractors(self):
        """åˆå§‹åŒ–æˆ–æ£€æŸ¥å¤–éƒ¨è§£æå™¨çš„å¯ç”¨æ€§ï¼Œç¼“å­˜åˆ°å®ä¾‹å±æ€§ã€‚"""
        if getattr(self, '_extractors_initialized', False):
            return
        self._extractors_initialized = True

        self.pdf_supported = False
        self.docx_supported = False
        self.pptx_supported = False
        self.odt_supported = False
        self.pdf_extractor = None
        self.docx_extractor = None

        # PDF extractor: prefer PyPDF2, fallback to pdfminer
        try:
            import PyPDF2
            # PyPDF2 may emit PdfReadWarning for some encodings (e.g. GBK-EUC-H).
            # Suppress those warnings to avoid noisy output during parsing.
            try:
                import warnings
                from PyPDF2.errors import PdfReadWarning

                warnings.filterwarnings("ignore", category=PdfReadWarning)
            except Exception:
                # best-effort: if specific warning class not available, ignore
                try:
                    import warnings

                    warnings.filterwarnings("ignore")
                except Exception:
                    pass

            def _extract_pdf_pypdf2(path):
                try:
                    reader = PyPDF2.PdfReader(path)
                    parts = []
                    for p in reader.pages:
                        try:
                            t = p.extract_text() or ''
                        except Exception:
                            t = ''
                        if t:
                            parts.append(t)
                    return '\n'.join(parts)
                except Exception:
                    return ''

            self.pdf_supported = True
            self.pdf_extractor = _extract_pdf_pypdf2
        except Exception:
            try:
                from pdfminer.high_level import extract_text as _pdfminer_extract_text

                def _extract_pdf_pdfminer(path):
                    try:
                        return _pdfminer_extract_text(path) or ''
                    except Exception:
                        return ''

                self.pdf_supported = True
                self.pdf_extractor = _extract_pdf_pdfminer
            except Exception:
                self.pdf_supported = False

        # docx
        try:
            import docx

            def _extract_docx(path):
                try:
                    doc = docx.Document(path)
                    return '\n'.join([p.text for p in doc.paragraphs])
                except Exception:
                    return ''

            self.docx_supported = True
            self.docx_extractor = _extract_docx
        except Exception:
            self.docx_supported = False

        # pptx (python-pptx)
        try:
            import pptx

            def _extract_pptx(path):
                try:
                    prs = pptx.Presentation(path)
                    parts = []
                    for slide in prs.slides:
                        for shape in slide.shapes:
                            if hasattr(shape, 'text'):
                                parts.append(shape.text)
                    return '\n'.join(parts)
                except Exception:
                    return ''

            self.pptx_supported = True
            self.pptx_extractor = _extract_pptx
        except Exception:
            self.pptx_supported = False

        # odt (odfpy)
        try:
            from odf import text as odf_text
            from odf import opendocument

            def _extract_odt(path):
                try:
                    doc = opendocument.load(path)
                    texts = []
                    for elem in doc.getElementsByType(odf_text.P):
                        texts.append(''.join(t.data for t in elem.childNodes if getattr(t, 'data', None)))
                    return '\n'.join(texts)
                except Exception:
                    return ''

            self.odt_supported = True
            self.odt_extractor = _extract_odt
        except Exception:
            self.odt_supported = False

    def check_parsers(self):
        """è¿”å›å½“å‰è§£æå™¨å¯ç”¨æ€§çš„å­—å…¸å’Œå»ºè®®çš„ pip å®‰è£…å‘½ä»¤å­—ç¬¦ä¸²ã€‚"""
        self._ensure_extractors()
        availability = {
            'pdf': bool(self.pdf_supported),
            'docx': bool(self.docx_supported),
            'pptx': bool(getattr(self, 'pptx_supported', False)),
            'odt': bool(getattr(self, 'odt_supported', False)),
        }
        # Suggest pip command (non-destructive): include common packages
        suggested = []
        if not availability['pdf']:
            suggested.append('PyPDF2')
            suggested.append('pdfminer.six')
        if not availability['docx']:
            suggested.append('python-docx')
        if not availability['pptx']:
            suggested.append('python-pptx')
        if not availability['odt']:
            suggested.append('odfpy')

        pip_cmd = 'pip install ' + ' '.join(sorted(set(suggested))) if suggested else ''
        return availability, pip_cmd

    def update_content_for_path(self, full_path, limit_size=10 * 1024 * 1024):
        """ä¸ºå•ä¸ªæ–‡ä»¶è§£æå¹¶æ›´æ–° `content_fts` ç´¢å¼•ï¼ˆé€’å¢æ›´æ–°ï¼‰ã€‚"""
        if not self.conn or not getattr(self, 'has_content_fts', False):
            logger.debug('æ— æ³•æ›´æ–°å†…å®¹ç´¢å¼•ï¼šæ•°æ®åº“æˆ– content_fts ä¸å¯ç”¨')
            return False

        self._ensure_extractors()

        try:
            with self.lock:
                cursor = self.conn.cursor()
                row = list(cursor.execute('SELECT id, extension, size FROM files WHERE full_path = ?', (full_path,)))
                if not row:
                    logger.debug(f'æ–‡ä»¶æœªåœ¨ç´¢å¼•ä¸­: {full_path}')
                    return False
                fid, ext, sz = row[0]

            if sz and sz > limit_size:
                logger.debug(f'è·³è¿‡å¤§æ–‡ä»¶ content index æ›´æ–°: {full_path}')
                return False

            ext_l = (ext or '').lower()
            text = ''
            # choose extractor based on extension
            if ext_l in {'.txt', '.md', '.py', '.csv', '.log', '.json', '.xml', '.html', '.htm', '.ini', '.cfg'}:
                try:
                    with open(full_path, 'rb') as f:
                        data = f.read()
                    try:
                        text = data.decode('utf-8')
                    except Exception:
                        try:
                            text = data.decode('gbk', errors='ignore')
                        except Exception:
                            text = data.decode('utf-8', errors='ignore')
                except Exception:
                    return False
            elif ext_l == '.pdf' and self.pdf_supported and self.pdf_extractor:
                try:
                    text = self.pdf_extractor(full_path) or ''
                except Exception:
                    return False
            elif ext_l == '.docx' and self.docx_supported and self.docx_extractor:
                try:
                    text = self.docx_extractor(full_path) or ''
                except Exception:
                    return False
            elif ext_l == '.pptx' and getattr(self, 'pptx_supported', False):
                try:
                    text = self.pptx_extractor(full_path) or ''
                except Exception:
                    return False
            elif ext_l == '.odt' and getattr(self, 'odt_supported', False):
                try:
                    text = self.odt_extractor(full_path) or ''
                except Exception:
                    return False
            else:
                # unsupported
                return False

            if not text:
                # nothing to index
                return False

            if len(text) > 1000000:
                text = text[:1000000]

            with self.lock:
                cursor = self.conn.cursor()
                try:
                    cursor.execute(
                        'INSERT OR REPLACE INTO content_fts(rowid, content, path, fileid) VALUES (?, ?, ?, ?)',
                        (fid, text, full_path, fid),
                    )
                except Exception:
                    try:
                        cursor.execute('DELETE FROM content_fts WHERE rowid = ?', (fid,))
                        cursor.execute(
                            'INSERT INTO content_fts(rowid, content, path, fileid) VALUES (?, ?, ?, ?)',
                            (fid, text, full_path, fid),
                        )
                    except Exception:
                        logger.debug(f'æ— æ³•å†™å…¥ content_fts: {full_path}')
                        return False
                if not HAS_APSW:
                    try:
                        self.conn.commit()
                    except Exception:
                        pass

            logger.info(f'âœ… content_fts æ›´æ–°: {full_path}')
            return True
        except Exception as e:
            logger.debug(f'update_content_for_path é”™è¯¯: {e}')
            return False

    def build_content_index(self, allowed_exts=None, limit_size=10 * 1024 * 1024):
        """æ„å»º/æ›´æ–°æ–‡ä»¶å†…å®¹çš„ FTS ç´¢å¼•ï¼ˆä»…å¯¹æ”¯æŒçš„æ–‡æœ¬æ‰©å±•åï¼‰ã€‚

        - `allowed_exts`: å¯é€‰çš„æ‰©å±•ååˆ—è¡¨ï¼ˆå¦‚ ['.txt', '.md']ï¼‰ï¼ŒæœªæŒ‡å®šåˆ™ä½¿ç”¨å†…ç½®æ–‡æœ¬æ‰©å±•é›†åˆã€‚
        - `limit_size`: è·³è¿‡è¶…è¿‡è¯¥å¤§å°ï¼ˆå­—èŠ‚ï¼‰çš„æ–‡ä»¶ï¼Œé»˜è®¤ 10MBã€‚
        """
        if not self.conn:
            logger.warning("æ— æ³•æ„å»ºå†…å®¹ç´¢å¼•ï¼šæ•°æ®åº“ä¸å¯ç”¨")
            return
        if not getattr(self, 'has_content_fts', False):
            logger.warning("å†…å®¹ FTS5 ä¸å¯ç”¨ï¼Œè·³è¿‡æ„å»ºå†…å®¹ç´¢å¼•")
            return

        text_exts = {'.txt', '.md', '.py', '.csv', '.log', '.json', '.xml', '.html', '.htm', '.ini', '.cfg'}
        pdf_exts = {'.pdf'}
        docx_exts = {'.docx'}

        if allowed_exts:
            allowed = set([e if e.startswith('.') else '.' + e for e in allowed_exts])
            text_exts = set(ext for ext in text_exts if ext in allowed)
            pdf_exts = set(ext for ext in pdf_exts if ext in allowed)
            docx_exts = set(ext for ext in docx_exts if ext in allowed)

        # ensure extractors are ready
        self._ensure_extractors()

        logger.info(
            f"å¼€å§‹æ„å»ºå†…å®¹ç´¢å¼•ï¼Œæ–‡æœ¬æ‰©å±•: {sorted(list(text_exts))}, pdf: {bool(pdf_exts)}, docx: {bool(docx_exts)}, å¤§å°ä¸Šé™: {limit_size} bytes"
        )

        # æ”¶é›†å€™é€‰æ–‡ä»¶ï¼ˆåœ¨é”å†…è¯»å–è¡¨ï¼‰
        candidates = []
        with self.lock:
            cursor = self.conn.cursor()
            try:
                for row in cursor.execute("SELECT id, full_path, extension, size FROM files"):
                    fid, fp, ext, sz = row
                    if not ext:
                        continue
                    ext_l = ext.lower()
                    if sz and sz > limit_size:
                        continue
                    # filter by allowed extensions
                    if ext_l in text_exts or ext_l in pdf_exts or ext_l in docx_exts:
                        candidates.append((fid, fp, ext_l, sz))
            except Exception:
                logger.debug('è¯»å– files åˆ—è¡¨å¤±è´¥')

        total = len(candidates)
        logger.info(f"å¾…è§£ææ–‡ä»¶æ•°: {total}")
        try:
            # emit initial content progress (parsed, written, total, msg)
            self.content_progress_signal.emit(0, 0, total, f"æ”¶é›†åˆ° {total} ä¸ªæ–‡ä»¶")
        except Exception:
            pass

        # worker to parse content for a single file
        def _parse_worker(item):
            fid, fp, ext_l, sz = item
            text = ''
            try:
                if ext_l in text_exts:
                    with open(fp, 'rb') as f:
                        data = f.read()
                    try:
                        text = data.decode('utf-8')
                    except Exception:
                        try:
                            text = data.decode('gbk', errors='ignore')
                        except Exception:
                            text = data.decode('utf-8', errors='ignore')
                elif ext_l in pdf_exts:
                    if not getattr(self, 'pdf_supported', False) or not getattr(self, 'pdf_extractor', None):
                        return None
                    text = self.pdf_extractor(fp) or ''
                elif ext_l in docx_exts:
                    if not getattr(self, 'docx_supported', False) or not getattr(self, 'docx_extractor', None):
                        return None
                    text = self.docx_extractor(fp) or ''
                else:
                    return None
            except FileNotFoundError:
                return None
            except PermissionError:
                return None
            except Exception:
                logger.debug(f'è§£æå¤±è´¥: {fp}')
                return None

            if not text:
                return None
            if len(text) > 1000000:
                text = text[:1000000]
            return (fid, fp, text)

        import concurrent.futures, multiprocessing
        max_workers = min(8, (multiprocessing.cpu_count() or 2) * 2)
        batch = []
        batch_size = 200

        parsed = 0
        written = 0
        canceled = False
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as ex:
            futures = {ex.submit(_parse_worker, item): item for item in candidates}
            for fut in concurrent.futures.as_completed(futures):
                # check for external stop request
                if getattr(self, '_stop_content_build', False):
                    logger.info('å†…å®¹ç´¢å¼•æ„å»ºå·²è¢«å–æ¶ˆ')
                    canceled = True
                    break
                res = None
                try:
                    res = fut.result()
                except Exception:
                    res = None
                if not res:
                    continue
                batch.append(res)
                if len(batch) >= batch_size:
                    # write batch to DB
                    with self.lock:
                        cursor = self.conn.cursor()
                        for fid, fp, text in batch:
                            try:
                                cursor.execute(
                                    "INSERT OR REPLACE INTO content_fts(rowid, content, path, fileid) VALUES (?, ?, ?, ?)",
                                    (fid, text, fp, fid),
                                )
                                written += 1
                            except Exception:
                                try:
                                    cursor.execute("DELETE FROM content_fts WHERE rowid = ?", (fid,))
                                    cursor.execute(
                                        "INSERT INTO content_fts(rowid, content, path, fileid) VALUES (?, ?, ?, ?)",
                                        (fid, text, fp, fid),
                                    )
                                except Exception:
                                    logger.debug(f"å†™å…¥ content_fts å¤±è´¥: {fp}")
                        try:
                            # emit content progress after write batch
                            parsed += len(batch)
                            self.content_progress_signal.emit(parsed, written, total, f"å†™å…¥ {written} æ¡ï¼Œæœ€è¿‘: {batch[-1][1]}")
                        except Exception:
                            pass
                        if not HAS_APSW:
                            try:
                                self.conn.commit()
                            except Exception:
                                pass
                    batch = []

        # flush remaining
        if batch:
            with self.lock:
                cursor = self.conn.cursor()
                for fid, fp, text in batch:
                    try:
                        cursor.execute(
                            "INSERT OR REPLACE INTO content_fts(rowid, content, path, fileid) VALUES (?, ?, ?, ?)",
                            (fid, text, fp, fid),
                        )
                        written += 1
                    except Exception:
                        try:
                            cursor.execute("DELETE FROM content_fts WHERE rowid = ?", (fid,))
                            cursor.execute(
                                "INSERT INTO content_fts(rowid, content, path, fileid) VALUES (?, ?, ?, ?)",
                                (fid, text, fp, fid),
                            )
                        except Exception:
                            logger.debug(f"å†™å…¥ content_fts å¤±è´¥: {fp}")
                if not HAS_APSW:
                    try:
                        self.conn.commit()
                    except Exception:
                        pass
            try:
                parsed += len(batch)
                self.content_progress_signal.emit(parsed, written, total, f"å®Œæˆå†™å…¥å‰©ä½™ {len(batch)} æ¡ï¼Œæœ€è¿‘: {batch[-1][1]}")
            except Exception:
                pass
        logger.info("å†…å®¹ç´¢å¼•æ„å»ºå®Œæˆ")
        try:
            self.content_progress_signal.emit(parsed, written, total, "å†…å®¹ç´¢å¼•æ„å»ºå®Œæˆ")
        except Exception:
            pass
        try:
            # emit finished/canceled signal
            self.content_build_finished_signal.emit(bool(canceled))
        except Exception:
            pass

    def stop_build_content(self):
        """è¯·æ±‚ä¸­æ–­æ­£åœ¨è¿›è¡Œçš„å†…å®¹ç´¢å¼•æ„å»ºï¼ˆçº¿ç¨‹å®‰å…¨æ ‡å¿—ï¼‰ã€‚"""
        self._stop_content_build = True

    def clear_stop_build(self):
        self._stop_content_build = False

    def clear_content_fts(self):
        """åˆ é™¤ `content_fts` è¡¨ä¸­æ‰€æœ‰æ¡ç›®ï¼ˆç”¨äºå›æ»š/æ¸…ç†ï¼‰ã€‚"""
        if not self.conn or not getattr(self, 'has_content_fts', False):
            logger.warning('æ— æ³•æ¸…ç† content_ftsï¼šæ•°æ®åº“æˆ– content_fts ä¸å¯ç”¨')
            return False
        try:
            with self.lock:
                cursor = self.conn.cursor()
                cursor.execute('DELETE FROM content_fts')
                if not HAS_APSW:
                    try:
                        self.conn.commit()
                    except Exception:
                        pass
            logger.info('âœ… content_fts å·²æ¸…ç†')
            return True
        except Exception as e:
            logger.error(f'æ¸…ç† content_fts å¤±è´¥: {e}')
            return False


__all__ = ["IndexManager"]
